{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ep_forecasting_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sympy import false, true\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Import \n",
    "\n",
    "# Load X_train\n",
    "with open(\"data/X_train.pkl\", \"rb\") as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "# Load y_train\n",
    "with open(\"data/y_train.pkl\", \"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "# Load X_val\n",
    "with open(\"data/X_val.pkl\", \"rb\") as f:\n",
    "    X_val = pickle.load(f)\n",
    "\n",
    "# Load y_val\n",
    "with open(\"data/y_val.pkl\", \"rb\") as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "# Load X_test\n",
    "with open(\"data/X_test.pkl\", \"rb\") as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "# Load y_test\n",
    "with open(\"data/y_test.pkl\", \"rb\") as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "# Load df_final_viz\n",
    "with open(\"data/df_final_viz.pkl\", \"rb\") as f:\n",
    "    df_final_viz = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch nutzt 8 Threads\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(torch.get_num_threads())  # Nutzt standardmäßig alle verfügbaren Kerne\n",
    "print(f\"PyTorch nutzt {torch.get_num_threads()} Threads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Torch-Tensoren umwandeln\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Datasets und DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last = True, num_workers = 8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last = True, num_workers = 8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last = True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hp):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hp['rnn_units'], batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(hp['dropout_rate_rnn'])\n",
    "        self.fc1 = nn.Linear(hp['rnn_units'], hp['dense_units'])\n",
    "        self.dropout2 = nn.Dropout(hp['dropout_rate_dense'])\n",
    "        self.fc2 = nn.Linear(hp['dense_units'], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # Nur der letzte Zeitschritt\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 12:51:48,366] A new study created in memory with name: no-name-48d7f4e7-1f3d-4e65-bef0-4f6e46c9807d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Train Loss: 0.0056, Val Loss: 0.0028\n",
      "Epoch [2/15], Train Loss: 0.0026, Val Loss: 0.0031\n",
      "Epoch [3/15], Train Loss: 0.0028, Val Loss: 0.0026\n",
      "Epoch [4/15], Train Loss: 0.0029, Val Loss: 0.0018\n",
      "Epoch [1/15], Train Loss: 0.0080, Val Loss: 0.0019\n",
      "Epoch [1/15], Train Loss: 0.0221, Val Loss: 0.0066\n",
      "Epoch [1/15], Train Loss: 0.0210, Val Loss: 0.0045\n",
      "Epoch [1/15], Train Loss: 0.0029, Val Loss: 0.0019\n",
      "Epoch [1/15], Train Loss: 0.0119, Val Loss: 0.0018\n",
      "Epoch [5/15], Train Loss: 0.0029, Val Loss: 0.0016\n",
      "Epoch [1/15], Train Loss: 0.0034, Val Loss: 0.0011\n",
      "Epoch [1/15], Train Loss: 0.0044, Val Loss: 0.0017\n",
      "Epoch [6/15], Train Loss: 0.0030, Val Loss: 0.0028\n",
      "Epoch [7/15], Train Loss: 0.0030, Val Loss: 0.0026\n",
      "Epoch [8/15], Train Loss: 0.0030, Val Loss: 0.0021\n",
      "Epoch [9/15], Train Loss: 0.0030, Val Loss: 0.0022\n",
      "Epoch [2/15], Train Loss: 0.0029, Val Loss: 0.0011\n",
      "Epoch [2/15], Train Loss: 0.0069, Val Loss: 0.0027\n",
      "Epoch [2/15], Train Loss: 0.0057, Val Loss: 0.0021\n",
      "Epoch [2/15], Train Loss: 0.0014, Val Loss: 0.0014\n",
      "Epoch [2/15], Train Loss: 0.0019, Val Loss: 0.0010\n",
      "Epoch [2/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0030, Val Loss: 0.0019\n",
      "Epoch [2/15], Train Loss: 0.0020, Val Loss: 0.0012\n",
      "Epoch [11/15], Train Loss: 0.0030, Val Loss: 0.0012\n",
      "Epoch [12/15], Train Loss: 0.0030, Val Loss: 0.0016\n",
      "Epoch [13/15], Train Loss: 0.0029, Val Loss: 0.0021\n",
      "Epoch [14/15], Train Loss: 0.0030, Val Loss: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 12:54:19,079] Trial 1 finished with value: 0.0010795074166737558 and parameters: {'rnn_units': 16, 'dropout_rate_rnn': 0.2, 'dense_units': 48, 'dropout_rate_dense': 0.4, 'learning_rate': 0.01, 'weight_decay': 0.001}. Best is trial 1 with value: 0.0010795074166737558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0031, Val Loss: 0.0011\n",
      "Epoch [3/15], Train Loss: 0.0018, Val Loss: 0.0007\n",
      "Epoch [3/15], Train Loss: 0.0049, Val Loss: 0.0023\n",
      "Epoch [3/15], Train Loss: 0.0045, Val Loss: 0.0018\n",
      "Epoch [3/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [3/15], Train Loss: 0.0017, Val Loss: 0.0008\n",
      "Epoch [3/15], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [3/15], Train Loss: 0.0016, Val Loss: 0.0015\n",
      "Epoch [1/15], Train Loss: 0.0349, Val Loss: 0.0056\n",
      "Epoch [4/15], Train Loss: 0.0014, Val Loss: 0.0007\n",
      "Epoch [4/15], Train Loss: 0.0039, Val Loss: 0.0019\n",
      "Epoch [4/15], Train Loss: 0.0039, Val Loss: 0.0017\n",
      "Epoch [4/15], Train Loss: 0.0013, Val Loss: 0.0010\n",
      "Epoch [4/15], Train Loss: 0.0017, Val Loss: 0.0011\n",
      "Epoch [4/15], Train Loss: 0.0009, Val Loss: 0.0008\n",
      "Epoch [4/15], Train Loss: 0.0015, Val Loss: 0.0011\n",
      "Epoch [2/15], Train Loss: 0.0100, Val Loss: 0.0019\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [5/15], Train Loss: 0.0033, Val Loss: 0.0023\n",
      "Epoch [5/15], Train Loss: 0.0035, Val Loss: 0.0017\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [5/15], Train Loss: 0.0018, Val Loss: 0.0015\n",
      "Epoch [5/15], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [5/15], Train Loss: 0.0014, Val Loss: 0.0011\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [3/15], Train Loss: 0.0079, Val Loss: 0.0016\n",
      "Epoch [6/15], Train Loss: 0.0028, Val Loss: 0.0017\n",
      "Epoch [6/15], Train Loss: 0.0031, Val Loss: 0.0014\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0009\n",
      "Epoch [6/15], Train Loss: 0.0019, Val Loss: 0.0014\n",
      "Epoch [6/15], Train Loss: 0.0009, Val Loss: 0.0008\n",
      "Epoch [6/15], Train Loss: 0.0014, Val Loss: 0.0010\n",
      "Epoch [7/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [4/15], Train Loss: 0.0070, Val Loss: 0.0015\n",
      "Epoch [7/15], Train Loss: 0.0025, Val Loss: 0.0016\n",
      "Epoch [7/15], Train Loss: 0.0028, Val Loss: 0.0015\n",
      "Epoch [7/15], Train Loss: 0.0020, Val Loss: 0.0011\n",
      "Epoch [7/15], Train Loss: 0.0013, Val Loss: 0.0010\n",
      "Epoch [7/15], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [7/15], Train Loss: 0.0014, Val Loss: 0.0009\n",
      "Epoch [8/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [8/15], Train Loss: 0.0023, Val Loss: 0.0015\n",
      "Epoch [5/15], Train Loss: 0.0063, Val Loss: 0.0020\n",
      "Epoch [8/15], Train Loss: 0.0026, Val Loss: 0.0016\n",
      "Epoch [8/15], Train Loss: 0.0020, Val Loss: 0.0014\n",
      "Epoch [8/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [8/15], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [8/15], Train Loss: 0.0014, Val Loss: 0.0010\n",
      "Epoch [9/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0020, Val Loss: 0.0017\n",
      "Epoch [6/15], Train Loss: 0.0056, Val Loss: 0.0014\n",
      "Epoch [9/15], Train Loss: 0.0024, Val Loss: 0.0013\n",
      "Epoch [9/15], Train Loss: 0.0022, Val Loss: 0.0012\n",
      "Epoch [9/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [9/15], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0014, Val Loss: 0.0012\n",
      "Epoch [10/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0019, Val Loss: 0.0011\n",
      "Epoch [7/15], Train Loss: 0.0050, Val Loss: 0.0015\n",
      "Epoch [10/15], Train Loss: 0.0022, Val Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 12:59:12,300] Trial 7 finished with value: 0.0008346766674461128 and parameters: {'rnn_units': 112, 'dropout_rate_rnn': 0.5, 'dense_units': 56, 'dropout_rate_dense': 0.0, 'learning_rate': 0.01, 'weight_decay': 0.0001}. Best is trial 7 with value: 0.0008346766674461128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Train Loss: 0.0023, Val Loss: 0.0018\n",
      "Early stopping nach 10 Epochen.\n",
      "Epoch [10/15], Train Loss: 0.0013, Val Loss: 0.0010\n",
      "Epoch [1/15], Train Loss: 0.0123, Val Loss: 0.0016\n",
      "Epoch [10/15], Train Loss: 0.0009, Val Loss: 0.0010\n",
      "Epoch [2/15], Train Loss: 0.0033, Val Loss: 0.0012\n",
      "Epoch [10/15], Train Loss: 0.0014, Val Loss: 0.0009\n",
      "Epoch [3/15], Train Loss: 0.0023, Val Loss: 0.0011\n",
      "Epoch [4/15], Train Loss: 0.0019, Val Loss: 0.0008\n",
      "Epoch [11/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [5/15], Train Loss: 0.0016, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0018, Val Loss: 0.0012\n",
      "Epoch [6/15], Train Loss: 0.0015, Val Loss: 0.0008\n",
      "Epoch [8/15], Train Loss: 0.0046, Val Loss: 0.0013\n",
      "Epoch [11/15], Train Loss: 0.0021, Val Loss: 0.0011\n",
      "Epoch [7/15], Train Loss: 0.0014, Val Loss: 0.0008\n",
      "Epoch [11/15], Train Loss: 0.0013, Val Loss: 0.0014\n",
      "Epoch [8/15], Train Loss: 0.0014, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0014, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0014, Val Loss: 0.0014\n",
      "Epoch [10/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0017, Val Loss: 0.0013\n",
      "Epoch [12/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0020, Val Loss: 0.0014\n",
      "Epoch [9/15], Train Loss: 0.0041, Val Loss: 0.0012\n",
      "Epoch [13/15], Train Loss: 0.0013, Val Loss: 0.0006\n",
      "Epoch [12/15], Train Loss: 0.0012, Val Loss: 0.0012\n",
      "Epoch [14/15], Train Loss: 0.0013, Val Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:01:25,180] Trial 9 finished with value: 0.0006393669800966182 and parameters: {'rnn_units': 16, 'dropout_rate_rnn': 0.2, 'dense_units': 56, 'dropout_rate_dense': 0.4, 'learning_rate': 0.001, 'weight_decay': 1e-05}. Best is trial 9 with value: 0.0006393669800966182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0014, Val Loss: 0.0014\n",
      "Epoch [13/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [13/15], Train Loss: 0.0016, Val Loss: 0.0012\n",
      "Epoch [13/15], Train Loss: 0.0019, Val Loss: 0.0011\n",
      "Epoch [10/15], Train Loss: 0.0036, Val Loss: 0.0012\n",
      "Epoch [13/15], Train Loss: 0.0012, Val Loss: 0.0010\n",
      "Epoch [1/15], Train Loss: 0.0178, Val Loss: 0.0036\n",
      "Epoch [13/15], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [13/15], Train Loss: 0.0014, Val Loss: 0.0009\n",
      "Epoch [14/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [14/15], Train Loss: 0.0016, Val Loss: 0.0009\n",
      "Epoch [14/15], Train Loss: 0.0018, Val Loss: 0.0010\n",
      "Epoch [11/15], Train Loss: 0.0033, Val Loss: 0.0011\n",
      "Epoch [14/15], Train Loss: 0.0012, Val Loss: 0.0009\n",
      "Epoch [2/15], Train Loss: 0.0058, Val Loss: 0.0018\n",
      "Epoch [14/15], Train Loss: 0.0009, Val Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:02:59,304] Trial 3 finished with value: 0.0008815942814356775 and parameters: {'rnn_units': 128, 'dropout_rate_rnn': 0.1, 'dense_units': 40, 'dropout_rate_dense': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.001}. Best is trial 9 with value: 0.0006393669800966182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15], Train Loss: 0.0014, Val Loss: 0.0011\n",
      "Early stopping nach 14 Epochen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:03:02,318] Trial 6 finished with value: 0.0006140163812585431 and parameters: {'rnn_units': 112, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 16, 'dropout_rate_dense': 0.2, 'learning_rate': 0.001, 'weight_decay': 1e-05}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0012, Val Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:03:09,892] Trial 4 finished with value: 0.0009394698036856191 and parameters: {'rnn_units': 80, 'dropout_rate_rnn': 0.5, 'dense_units': 56, 'dropout_rate_dense': 0.0, 'learning_rate': 0.0001, 'weight_decay': 0.001}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0015, Val Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:03:14,649] Trial 2 finished with value: 0.0009985754056082618 and parameters: {'rnn_units': 80, 'dropout_rate_rnn': 0.2, 'dense_units': 40, 'dropout_rate_dense': 0.2, 'learning_rate': 0.0001, 'weight_decay': 0.001}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0017, Val Loss: 0.0010\n",
      "Epoch [12/15], Train Loss: 0.0029, Val Loss: 0.0011\n",
      "Epoch [1/15], Train Loss: 0.1099, Val Loss: 0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:03:27,332] Trial 0 finished with value: 0.000852664282231877 and parameters: {'rnn_units': 112, 'dropout_rate_rnn': 0.1, 'dense_units': 40, 'dropout_rate_dense': 0.0, 'learning_rate': 0.001, 'weight_decay': 0.001}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0012, Val Loss: 0.0009\n",
      "Early stopping nach 15 Epochen.\n",
      "Epoch [2/15], Train Loss: 0.0461, Val Loss: 0.0135\n",
      "Epoch [3/15], Train Loss: 0.0047, Val Loss: 0.0016\n",
      "Epoch [3/15], Train Loss: 0.0335, Val Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:03:47,538] Trial 5 finished with value: 0.0006246793794073915 and parameters: {'rnn_units': 128, 'dropout_rate_rnn': 0.1, 'dense_units': 64, 'dropout_rate_dense': 0.0, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [1/15], Train Loss: 0.0122, Val Loss: 0.0024\n",
      "Epoch [4/15], Train Loss: 0.0269, Val Loss: 0.0099\n",
      "Epoch [1/15], Train Loss: 0.0505, Val Loss: 0.0083\n",
      "Epoch [1/15], Train Loss: 0.0308, Val Loss: 0.0042\n",
      "Epoch [5/15], Train Loss: 0.0223, Val Loss: 0.0070\n",
      "Epoch [2/15], Train Loss: 0.0133, Val Loss: 0.0061\n",
      "Epoch [1/15], Train Loss: 0.0090, Val Loss: 0.0016\n",
      "Epoch [6/15], Train Loss: 0.0182, Val Loss: 0.0054\n",
      "Epoch [13/15], Train Loss: 0.0027, Val Loss: 0.0009\n",
      "Epoch [3/15], Train Loss: 0.0092, Val Loss: 0.0052\n",
      "Epoch [7/15], Train Loss: 0.0147, Val Loss: 0.0034\n",
      "Epoch [4/15], Train Loss: 0.0062, Val Loss: 0.0031\n",
      "Epoch [1/15], Train Loss: 0.0078, Val Loss: 0.0013\n",
      "Epoch [8/15], Train Loss: 0.0120, Val Loss: 0.0033\n",
      "Epoch [5/15], Train Loss: 0.0045, Val Loss: 0.0024\n",
      "Epoch [9/15], Train Loss: 0.0100, Val Loss: 0.0028\n",
      "Epoch [4/15], Train Loss: 0.0040, Val Loss: 0.0016\n",
      "Epoch [6/15], Train Loss: 0.0037, Val Loss: 0.0021\n",
      "Epoch [10/15], Train Loss: 0.0083, Val Loss: 0.0030\n",
      "Epoch [2/15], Train Loss: 0.0048, Val Loss: 0.0021\n",
      "Epoch [7/15], Train Loss: 0.0032, Val Loss: 0.0019\n",
      "Epoch [11/15], Train Loss: 0.0070, Val Loss: 0.0021\n",
      "Epoch [2/15], Train Loss: 0.0121, Val Loss: 0.0047\n",
      "Epoch [8/15], Train Loss: 0.0030, Val Loss: 0.0018\n",
      "Epoch [12/15], Train Loss: 0.0060, Val Loss: 0.0025\n",
      "Epoch [2/15], Train Loss: 0.0017, Val Loss: 0.0009\n",
      "Epoch [9/15], Train Loss: 0.0028, Val Loss: 0.0017\n",
      "Epoch [13/15], Train Loss: 0.0052, Val Loss: 0.0020\n",
      "Epoch [14/15], Train Loss: 0.0024, Val Loss: 0.0010\n",
      "Epoch [10/15], Train Loss: 0.0026, Val Loss: 0.0016\n",
      "Epoch [14/15], Train Loss: 0.0046, Val Loss: 0.0020\n",
      "Epoch [2/15], Train Loss: 0.0029, Val Loss: 0.0018\n",
      "Epoch [11/15], Train Loss: 0.0025, Val Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:06:11,736] Trial 14 finished with value: 0.001976413135003719 and parameters: {'rnn_units': 16, 'dropout_rate_rnn': 0.2, 'dense_units': 16, 'dropout_rate_dense': 0.4, 'learning_rate': 0.0001, 'weight_decay': 0.0001}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0041, Val Loss: 0.0023\n",
      "Epoch [12/15], Train Loss: 0.0023, Val Loss: 0.0013\n",
      "Epoch [5/15], Train Loss: 0.0036, Val Loss: 0.0014\n",
      "Epoch [13/15], Train Loss: 0.0023, Val Loss: 0.0013\n",
      "Epoch [3/15], Train Loss: 0.0040, Val Loss: 0.0020\n",
      "Epoch [1/15], Train Loss: 0.0277, Val Loss: 0.0029\n",
      "Epoch [14/15], Train Loss: 0.0022, Val Loss: 0.0012\n",
      "Epoch [3/15], Train Loss: 0.0101, Val Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:06:52,028] Trial 16 finished with value: 0.0011425041686321023 and parameters: {'rnn_units': 32, 'dropout_rate_rnn': 0.4, 'dense_units': 64, 'dropout_rate_dense': 0.0, 'learning_rate': 0.0001, 'weight_decay': 0.0001}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0021, Val Loss: 0.0011\n",
      "Epoch [3/15], Train Loss: 0.0015, Val Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:06:58,396] Trial 8 finished with value: 0.0008801198357926501 and parameters: {'rnn_units': 112, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 24, 'dropout_rate_dense': 0.2, 'learning_rate': 0.0001, 'weight_decay': 1e-05}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0022, Val Loss: 0.0009\n",
      "Epoch [2/15], Train Loss: 0.0069, Val Loss: 0.0018\n",
      "Epoch [3/15], Train Loss: 0.0020, Val Loss: 0.0010\n",
      "Epoch [3/15], Train Loss: 0.0034, Val Loss: 0.0012\n",
      "Epoch [6/15], Train Loss: 0.0032, Val Loss: 0.0013\n",
      "Epoch [4/15], Train Loss: 0.0034, Val Loss: 0.0017\n",
      "Epoch [4/15], Train Loss: 0.0087, Val Loss: 0.0017\n",
      "Epoch [4/15], Train Loss: 0.0026, Val Loss: 0.0010\n",
      "Epoch [1/15], Train Loss: 0.0101, Val Loss: 0.0013\n",
      "Epoch [4/15], Train Loss: 0.0016, Val Loss: 0.0010\n",
      "Epoch [1/15], Train Loss: 0.0091, Val Loss: 0.0019\n",
      "Epoch [4/15], Train Loss: 0.0015, Val Loss: 0.0010\n",
      "Epoch [5/15], Train Loss: 0.0024, Val Loss: 0.0008\n",
      "Epoch [7/15], Train Loss: 0.0028, Val Loss: 0.0012\n",
      "Epoch [6/15], Train Loss: 0.0022, Val Loss: 0.0008\n",
      "Epoch [5/15], Train Loss: 0.0031, Val Loss: 0.0024\n",
      "Epoch [5/15], Train Loss: 0.0077, Val Loss: 0.0014\n",
      "Epoch [7/15], Train Loss: 0.0021, Val Loss: 0.0006\n",
      "Epoch [2/15], Train Loss: 0.0039, Val Loss: 0.0013\n",
      "Epoch [2/15], Train Loss: 0.0034, Val Loss: 0.0010\n",
      "Epoch [5/15], Train Loss: 0.0017, Val Loss: 0.0012\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [8/15], Train Loss: 0.0020, Val Loss: 0.0007\n",
      "Epoch [8/15], Train Loss: 0.0026, Val Loss: 0.0011\n",
      "Epoch [6/15], Train Loss: 0.0028, Val Loss: 0.0017\n",
      "Epoch [9/15], Train Loss: 0.0020, Val Loss: 0.0008\n",
      "Epoch [6/15], Train Loss: 0.0067, Val Loss: 0.0022\n",
      "Epoch [3/15], Train Loss: 0.0025, Val Loss: 0.0009\n",
      "Epoch [10/15], Train Loss: 0.0020, Val Loss: 0.0006\n",
      "Epoch [3/15], Train Loss: 0.0020, Val Loss: 0.0007\n",
      "Epoch [6/15], Train Loss: 0.0019, Val Loss: 0.0013\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [11/15], Train Loss: 0.0020, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0024, Val Loss: 0.0012\n",
      "Epoch [7/15], Train Loss: 0.0026, Val Loss: 0.0016\n",
      "Epoch [12/15], Train Loss: 0.0020, Val Loss: 0.0007\n",
      "Epoch [7/15], Train Loss: 0.0059, Val Loss: 0.0017\n",
      "Epoch [4/15], Train Loss: 0.0017, Val Loss: 0.0010\n",
      "Epoch [4/15], Train Loss: 0.0014, Val Loss: 0.0007\n",
      "Epoch [13/15], Train Loss: 0.0020, Val Loss: 0.0007\n",
      "Epoch [7/15], Train Loss: 0.1128, Val Loss: 0.0126\n",
      "Epoch [7/15], Train Loss: 0.0012, Val Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:10:42,420] Trial 17 finished with value: 0.0006328029518748911 and parameters: {'rnn_units': 48, 'dropout_rate_rnn': 0.4, 'dense_units': 8, 'dropout_rate_dense': 0.30000000000000004, 'learning_rate': 0.001, 'weight_decay': 1e-05}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15], Train Loss: 0.0019, Val Loss: 0.0008\n",
      "Early stopping nach 14 Epochen.\n",
      "Epoch [10/15], Train Loss: 0.0022, Val Loss: 0.0012\n",
      "Epoch [8/15], Train Loss: 0.0024, Val Loss: 0.0013\n",
      "Epoch [5/15], Train Loss: 0.0014, Val Loss: 0.0007\n",
      "Epoch [8/15], Train Loss: 0.0051, Val Loss: 0.0011\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [8/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [8/15], Train Loss: 0.0188, Val Loss: 0.0119\n",
      "Epoch [1/15], Train Loss: 0.0061, Val Loss: 0.0013\n",
      "Epoch [11/15], Train Loss: 0.0020, Val Loss: 0.0010\n",
      "Epoch [9/15], Train Loss: 0.0022, Val Loss: 0.0017\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [9/15], Train Loss: 0.0045, Val Loss: 0.0012\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [9/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0148, Val Loss: 0.0106\n",
      "Epoch [2/15], Train Loss: 0.0023, Val Loss: 0.0012\n",
      "Epoch [12/15], Train Loss: 0.0019, Val Loss: 0.0009\n",
      "Epoch [10/15], Train Loss: 0.0021, Val Loss: 0.0013\n",
      "Epoch [7/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [7/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0039, Val Loss: 0.0011\n",
      "Epoch [10/15], Train Loss: 0.0011, Val Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:12:52,549] Trial 13 finished with value: 0.0008512251570796399 and parameters: {'rnn_units': 128, 'dropout_rate_rnn': 0.4, 'dense_units': 56, 'dropout_rate_dense': 0.2, 'learning_rate': 0.01, 'weight_decay': 1e-05}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Train Loss: 0.0119, Val Loss: 0.0047\n",
      "Early stopping nach 10 Epochen.\n",
      "Epoch [3/15], Train Loss: 0.0016, Val Loss: 0.0008\n",
      "Epoch [11/15], Train Loss: 0.0019, Val Loss: 0.0013\n",
      "Epoch [13/15], Train Loss: 0.0018, Val Loss: 0.0008\n",
      "Epoch [8/15], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [8/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0035, Val Loss: 0.0011\n",
      "Epoch [11/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [1/15], Train Loss: 0.0038, Val Loss: 0.0013\n",
      "Epoch [12/15], Train Loss: 0.0018, Val Loss: 0.0013\n",
      "Epoch [14/15], Train Loss: 0.0017, Val Loss: 0.0009\n",
      "Epoch [4/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0031, Val Loss: 0.0012\n",
      "Epoch [2/15], Train Loss: 0.0017, Val Loss: 0.0008\n",
      "Epoch [13/15], Train Loss: 0.0018, Val Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:14:33,842] Trial 10 finished with value: 0.0008140040104814445 and parameters: {'rnn_units': 112, 'dropout_rate_rnn': 0.4, 'dense_units': 32, 'dropout_rate_dense': 0.1, 'learning_rate': 0.0001, 'weight_decay': 1e-05}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0016, Val Loss: 0.0008\n",
      "Epoch [5/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [10/15], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [13/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [13/15], Train Loss: 0.0028, Val Loss: 0.0011\n",
      "Epoch [3/15], Train Loss: 0.0013, Val Loss: 0.0010\n",
      "Epoch [14/15], Train Loss: 0.0017, Val Loss: 0.0012\n",
      "Epoch [1/15], Train Loss: 0.0044, Val Loss: 0.0019\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [14/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [14/15], Train Loss: 0.0025, Val Loss: 0.0011\n",
      "Epoch [4/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [2/15], Train Loss: 0.0019, Val Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:15:57,881] Trial 11 finished with value: 0.00097119541210451 and parameters: {'rnn_units': 80, 'dropout_rate_rnn': 0.2, 'dense_units': 32, 'dropout_rate_dense': 0.30000000000000004, 'learning_rate': 0.0001, 'weight_decay': 0.001}. Best is trial 6 with value: 0.0006140163812585431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0016, Val Loss: 0.0010\n",
      "Epoch [7/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Epoch [12/15], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0011, Val Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:16:21,318] Trial 15 finished with value: 0.0005903245406854342 and parameters: {'rnn_units': 80, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 40, 'dropout_rate_dense': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 15 with value: 0.0005903245406854342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0011, Val Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:16:28,960] Trial 12 finished with value: 0.0010618649304829418 and parameters: {'rnn_units': 128, 'dropout_rate_rnn': 0.4, 'dense_units': 32, 'dropout_rate_dense': 0.30000000000000004, 'learning_rate': 0.0001, 'weight_decay': 1e-05}. Best is trial 15 with value: 0.0005903245406854342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0023, Val Loss: 0.0012\n",
      "Epoch [5/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Epoch [3/15], Train Loss: 0.0014, Val Loss: 0.0008\n",
      "Epoch [1/15], Train Loss: 0.0077, Val Loss: 0.0013\n",
      "Epoch [1/15], Train Loss: 0.0090, Val Loss: 0.0014\n",
      "Epoch [1/15], Train Loss: 0.0210, Val Loss: 0.0020\n",
      "Epoch [8/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [2/15], Train Loss: 0.0028, Val Loss: 0.0011\n",
      "Epoch [2/15], Train Loss: 0.0077, Val Loss: 0.0014\n",
      "Epoch [13/15], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [13/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [3/15], Train Loss: 0.0019, Val Loss: 0.0009\n",
      "Epoch [6/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [3/15], Train Loss: 0.0042, Val Loss: 0.0015\n",
      "Epoch [4/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [2/15], Train Loss: 0.0032, Val Loss: 0.0009\n",
      "Epoch [4/15], Train Loss: 0.0015, Val Loss: 0.0008\n",
      "Epoch [4/15], Train Loss: 0.0025, Val Loss: 0.0010\n",
      "Epoch [9/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:18:13,775] Trial 18 finished with value: 0.0005743369560913115 and parameters: {'rnn_units': 128, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 16, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 1e-05}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Early stopping nach 14 Epochen.\n",
      "Epoch [5/15], Train Loss: 0.0018, Val Loss: 0.0012\n",
      "Epoch [14/15], Train Loss: 0.0011, Val Loss: 0.0010\n",
      "Epoch [7/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [6/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [5/15], Train Loss: 0.0012, Val Loss: 0.0009\n",
      "Epoch [3/15], Train Loss: 0.0019, Val Loss: 0.0009\n",
      "Epoch [6/15], Train Loss: 0.0015, Val Loss: 0.0008\n",
      "Epoch [1/15], Train Loss: 0.0077, Val Loss: 0.0014\n",
      "Epoch [7/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [7/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [2/15], Train Loss: 0.0028, Val Loss: 0.0009\n",
      "Epoch [10/15], Train Loss: 0.0011, Val Loss: 0.0010\n",
      "Epoch [8/15], Train Loss: 0.0013, Val Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:19:18,698] Trial 19 finished with value: 0.0006143561339183268 and parameters: {'rnn_units': 128, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 8, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 1e-05}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [8/15], Train Loss: 0.0013, Val Loss: 0.0008\n",
      "Epoch [3/15], Train Loss: 0.0020, Val Loss: 0.0008\n",
      "Epoch [8/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [4/15], Train Loss: 0.0014, Val Loss: 0.0009\n",
      "Epoch [9/15], Train Loss: 0.0013, Val Loss: 0.0006\n",
      "Epoch [1/15], Train Loss: 0.0049, Val Loss: 0.0017\n",
      "Epoch [9/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [4/15], Train Loss: 0.0016, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0013, Val Loss: 0.0006\n",
      "Epoch [2/15], Train Loss: 0.0020, Val Loss: 0.0009\n",
      "Epoch [10/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0011\n",
      "Epoch [9/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [11/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [3/15], Train Loss: 0.0015, Val Loss: 0.0008\n",
      "Epoch [7/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0008\n",
      "Epoch [11/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [12/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [4/15], Train Loss: 0.0013, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [7/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0011, Val Loss: 0.0010\n",
      "Epoch [13/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [5/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [13/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [8/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Epoch [8/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [14/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [6/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [14/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0011, Val Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:22:34,738] Trial 20 finished with value: 0.0006809995961262875 and parameters: {'rnn_units': 128, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 24, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Train Loss: 0.0011, Val Loss: 0.0009\n",
      "Early stopping nach 13 Epochen.\n",
      "Epoch [15/15], Train Loss: 0.0012, Val Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:22:35,208] Trial 24 finished with value: 0.0006371874569595093 and parameters: {'rnn_units': 64, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 8, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15], Train Loss: 0.0011, Val Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:22:44,231] Trial 25 finished with value: 0.0006435765079063017 and parameters: {'rnn_units': 64, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 8, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [10/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [7/15], Train Loss: 0.0012, Val Loss: 0.0009\n",
      "Epoch [11/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Epoch [8/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [11/15], Train Loss: 0.0011, Val Loss: 0.0009\n",
      "Epoch [9/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [1/15], Train Loss: 0.0118, Val Loss: 0.0012\n",
      "Epoch [1/15], Train Loss: 0.0094, Val Loss: 0.0012\n",
      "Epoch [12/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [10/15], Train Loss: 0.0012, Val Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:23:47,448] Trial 26 finished with value: 0.0006287034429401354 and parameters: {'rnn_units': 64, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 24, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Early stopping nach 13 Epochen.\n",
      "Epoch [8/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [12/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Epoch [11/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [2/15], Train Loss: 0.0045, Val Loss: 0.0011\n",
      "Epoch [2/15], Train Loss: 0.0036, Val Loss: 0.0009\n",
      "Epoch [12/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [11/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [9/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [13/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Epoch [13/15], Train Loss: 0.0011, Val Loss: 0.0010\n",
      "Epoch [3/15], Train Loss: 0.0029, Val Loss: 0.0011\n",
      "Epoch [14/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [3/15], Train Loss: 0.0023, Val Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:24:56,299] Trial 27 finished with value: 0.0006295986669659126 and parameters: {'rnn_units': 64, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 24, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0011, Val Loss: 0.0008\n",
      "Early stopping nach 15 Epochen.\n",
      "Epoch [12/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0012, Val Loss: 0.0008\n",
      "Epoch [14/15], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [4/15], Train Loss: 0.0020, Val Loss: 0.0008\n",
      "Epoch [4/15], Train Loss: 0.0016, Val Loss: 0.0007\n",
      "Epoch [13/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [11/15], Train Loss: 0.0012, Val Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:25:28,908] Trial 21 finished with value: 0.0006421912859819244 and parameters: {'rnn_units': 96, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 24, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [5/15], Train Loss: 0.0015, Val Loss: 0.0007\n",
      "Epoch [5/15], Train Loss: 0.0013, Val Loss: 0.0009\n",
      "Epoch [14/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [6/15], Train Loss: 0.0013, Val Loss: 0.0008\n",
      "Epoch [6/15], Train Loss: 0.0012, Val Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:26:06,871] Trial 22 finished with value: 0.000631184100242418 and parameters: {'rnn_units': 96, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 8, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [13/15], Train Loss: 0.0012, Val Loss: 0.0009\n",
      "Epoch [7/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [7/15], Train Loss: 0.0011, Val Loss: 0.0006\n",
      "Epoch [14/15], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [8/15], Train Loss: 0.0011, Val Loss: 0.0012\n",
      "Epoch [8/15], Train Loss: 0.0011, Val Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:26:39,356] Trial 23 finished with value: 0.000626548415311845 and parameters: {'rnn_units': 96, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 8, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.0001}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0012, Val Loss: 0.0007\n",
      "Epoch [9/15], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [9/15], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [10/15], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [10/15], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [11/15], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [11/15], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [12/15], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [12/15], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [13/15], Train Loss: 0.0010, Val Loss: 0.0009\n",
      "Epoch [13/15], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [14/15], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [14/15], Train Loss: 0.0010, Val Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:27:50,906] Trial 29 finished with value: 0.0005941036032445188 and parameters: {'rnn_units': 96, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 16, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 1e-05}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0010, Val Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 13:27:51,215] Trial 28 finished with value: 0.0005934571509574345 and parameters: {'rnn_units': 96, 'dropout_rate_rnn': 0.30000000000000004, 'dense_units': 16, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001, 'weight_decay': 1e-05}. Best is trial 18 with value: 0.0005743369560913115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Best trial parameters:\n",
      "rnn_units: 128\n",
      "dropout_rate_rnn: 0.30000000000000004\n",
      "dense_units: 16\n",
      "dropout_rate_dense: 0.1\n",
      "learning_rate: 0.001\n",
      "weight_decay: 1e-05\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    hp = {\n",
    "        'rnn_units': trial.suggest_int('rnn_units', 16, 128, step=16),\n",
    "        'dropout_rate_rnn': trial.suggest_float('dropout_rate_rnn', 0.1, 0.5, step=0.1),\n",
    "        'dense_units': trial.suggest_int('dense_units', 8, 64, step=8),\n",
    "        'dropout_rate_dense': trial.suggest_float('dropout_rate_dense', 0.0, 0.4, step=0.1),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [1e-2, 1e-3, 1e-4]),\n",
    "        'weight_decay': trial.suggest_categorical('weight_decay', [1e-5, 1e-4, 1e-3]),\n",
    "    }\n",
    "\n",
    "    rnn_model = SimpleRNN(input_size = X_train.shape[2], hp= hp)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(rnn_model.parameters(), lr=hp['learning_rate'], weight_decay=hp['weight_decay'])\n",
    "\n",
    "    num_epochs = 15\n",
    "    patience = 7  # Stop, wenn val_loss sich 10 Epochen lang nicht verbessert\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        rnn_model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Training Loop\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = rnn_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation Loop (nach jeder Epoche)\n",
    "        rnn_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = rnn_model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0  # Reset Counter\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping nach {epoch+1} Epochen.\")\n",
    "                break\n",
    "\n",
    "    return best_val_loss  # Val Loss zurückgeben für Optuna\n",
    "\n",
    "# Hyperparameter tuning\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30, n_jobs = 8)\n",
    "\n",
    "# Show Best Result\n",
    "print(\"Best trial parameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best hyperparameters\n",
    "best_hp = study.best_trial.params\n",
    "with open(\"best_hp_all_models/best_hp_rnn.json\", \"w\") as f:\n",
    "    json.dump(best_hp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best hyperparameters \n",
    "with open(\"best_hp_all_models/best_hp_rnn.json\", \"r\") as f:\n",
    "    best_hp = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.0102, Val Loss: 0.0012\n",
      "Epoch [2/50], Train Loss: 0.0041, Val Loss: 0.0021\n",
      "Epoch [3/50], Train Loss: 0.0026, Val Loss: 0.0008\n",
      "Epoch [4/50], Train Loss: 0.0018, Val Loss: 0.0011\n",
      "Epoch [5/50], Train Loss: 0.0014, Val Loss: 0.0007\n",
      "Epoch [6/50], Train Loss: 0.0012, Val Loss: 0.0006\n",
      "Epoch [7/50], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [8/50], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [9/50], Train Loss: 0.0011, Val Loss: 0.0007\n",
      "Epoch [10/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [11/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [12/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [13/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [14/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [15/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [16/50], Train Loss: 0.0010, Val Loss: 0.0009\n",
      "Epoch [17/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [18/50], Train Loss: 0.0010, Val Loss: 0.0008\n",
      "Epoch [19/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [20/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [21/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [22/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [23/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [24/50], Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch [25/50], Train Loss: 0.0009, Val Loss: 0.0008\n",
      "Epoch [26/50], Train Loss: 0.0010, Val Loss: 0.0009\n",
      "Epoch [27/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [28/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [29/50], Train Loss: 0.0010, Val Loss: 0.0006\n",
      "Epoch [30/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [31/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [32/50], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [33/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [34/50], Train Loss: 0.0009, Val Loss: 0.0008\n",
      "Epoch [35/50], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [36/50], Train Loss: 0.0009, Val Loss: 0.0008\n",
      "Epoch [37/50], Train Loss: 0.0009, Val Loss: 0.0008\n",
      "Epoch [38/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Epoch [39/50], Train Loss: 0.0009, Val Loss: 0.0007\n",
      "Epoch [40/50], Train Loss: 0.0009, Val Loss: 0.0006\n",
      "Early stopping nach 40 Epochen.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bestes Modell mit den gefundenen Hyperparametern trainieren\n",
    "best_hp = study.best_trial.params\n",
    "final_model = SimpleRNN(input_size=X_train.shape[2], hp=best_hp)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_hp['learning_rate'], weight_decay=best_hp['weight_decay'])\n",
    "\n",
    "num_epochs = 50\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = final_model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Validation Loss berechnen\n",
    "    final_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_pred = final_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0  # Reset Counter\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping nach {epoch+1} Epochen.\")\n",
    "            break\n",
    "\n",
    "    # Save weights of the model \n",
    "    torch.save(final_model.state_dict(), \"saved_models/rnn_model_final.pth\")\n",
    "\n",
    "# **Trainingshistorie plotten**\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(val_loss_history, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochen\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Trainings- und Validierungsverlust\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (rnn): RNN(17, 128, batch_first=True)\n",
       "  (dropout1): Dropout(p=0.30000000000000004, inplace=False)\n",
       "  (fc1): Linear(in_features=128, out_features=16, bias=True)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model  \n",
    "rnn_final = SimpleRNN(input_size=X_train.shape[2], hp=best_hp)\n",
    "rnn_final.load_state_dict(torch.load(\"saved_models/rnn_model_final.pth\"))\n",
    "rnn_final.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 0.0005\n",
      "Test RMSE (PyTorch): 0.0224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rnn_final.eval()\n",
    "test_loss = 0\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_test_batch, y_test_batch in test_loader:\n",
    "        X_test_batch = X_test_batch\n",
    "        y_test_batch = y_test_batch\n",
    "\n",
    "        test_outputs = rnn_final(X_test_batch).squeeze()\n",
    "        loss = criterion(test_outputs, y_test_batch.squeeze())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        test_predictions.extend(test_outputs.cpu().numpy())\n",
    "        test_targets.extend(y_test_batch.squeeze().cpu().numpy())\n",
    "\n",
    "# Mittelwert des Kriteriums (z. B. MSELoss)\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "def RMSELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "# Torch-Tensors\n",
    "y_pred_tensor = torch.tensor(test_predictions)\n",
    "y_true_tensor = torch.tensor(test_targets)\n",
    "\n",
    "rmse = RMSELoss(y_pred_tensor, y_true_tensor).item()\n",
    "print(f\"Test RMSE (PyTorch): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ep_forecasting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
